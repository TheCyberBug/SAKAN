{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "1. ### <s> Lemmatize input - Good\n",
    "2. ### <s> Replace CNNs with KAN-CNNs - Won't learn, Revisit last\n",
    "3. ### Prune and test\n",
    "4. ### <s> B-spline Finetuning - Best with splines = 3\n",
    "5. ### CNN operates on whole embedded lvl - test on various depths (3, 9, 27, 81)\n",
    "6. ### Deeper CNN - 1 + 3 + 5 gram feed into Deep CNN -> KAN \n",
    "7. ### <s> Data Augmentation\n",
    "8. ### Limit Vocab Size\n",
    "9. ### Try Word2Vec or Glove pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from fastkan import FastKAN as KAN\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('./imdb.csv')\n",
    "#data.head()\n",
    "\n",
    "train = pd.read_csv(\"./data/imdb_train_original.csv\")\n",
    "test = pd.read_csv(\"./data/imdb_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train.head)\n",
    "# print(test.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to binary\n",
    "def transform_label(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "\n",
    "train['label'] = train['sentiment'].progress_apply(transform_label)\n",
    "test['label'] = test['sentiment'].progress_apply(transform_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for dataset imbalance\n",
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Review token length\n",
    "# train['token_length'] = train.review.progress_apply(lambda x: len(x.split()))\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pos = train[train['label'] == 1]\n",
    "# print(\"Positive review length\")\n",
    "# data_pos['token_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_neg = train[train['label'] == 0]\n",
    "# print(\"Negative review length\")\n",
    "# data_neg['token_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "    # \n",
    "    words = word_tokenize(text)\n",
    "    sent = [word for word in words if word not in stop_words]\n",
    "    sent = ' '.join(sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:18<00:00, 2135.26it/s]\n",
      "100%|██████████| 10000/10000 [00:04<00:00, 2146.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "train['clean'] = train.review.progress_apply(preprocess_text)\n",
    "test['clean'] = test.review.progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.head())\n",
    "# print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text if necessary\n",
    "    words = text.split()\n",
    "    # Lemmatize each word in the text\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the lemmatized words back into a string\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:11<00:00, 3466.13it/s]\n",
      "100%|██████████| 10000/10000 [00:02<00:00, 3727.94it/s]\n"
     ]
    }
   ],
   "source": [
    "train['lemma'] = train['clean'].progress_apply(lemmatize_text)\n",
    "test['lemma'] = test['clean'].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(20):\n",
    "#     print(data['clean'][n])\n",
    "#     print(data['lemma'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:23<00:00, 1675.56it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1755.91it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_l = 256\n",
    "#data['tokenized'] = data.lemma.progress_apply(lambda x: tokenizer.encode(text=x, add_special_tokens=False, truncation=True, add_prefix_space=True, padding='max_length', max_length=max_l))\n",
    "train['tokenized'] = train.lemma.progress_apply(lambda x: tokenizer.encode(text=x, add_special_tokens=False, truncation=True, add_prefix_space=True, padding='max_length', max_length=max_l))\n",
    "test['tokenized'] = test.lemma.progress_apply(lambda x: tokenizer.encode(text=x, add_special_tokens=False, truncation=True, add_prefix_space=True, padding='max_length', max_length=max_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check actual tokenized text len\n",
    "# data['ct_length'] = data.tokenized.progress_apply(lambda x: len(x))\n",
    "# data_pos = data[data['label'] == 1]\n",
    "# data_pos['ct_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "# X = data.tokenized\n",
    "# y = data.label\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)\n",
    "X_train = train.tokenized\n",
    "y_train = train.label\n",
    "\n",
    "X_test = test.tokenized\n",
    "y_test = test.label\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.tolist()).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.tolist()).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.tolist()).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.tolist()).to(device)\n",
    "\n",
    "# Create Datasets - tensors\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the data\n",
    "batch_size = 96\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# BEST__SMALL\n",
    "# Model 89.38% ACC\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Conv layers with different kernel sizes\n",
    "        self.conv1 = nn.Conv2d(1, 256, (1, embed_dim), padding=(0, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1, 256, (3, embed_dim), padding=(1, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(1, 256, (5, embed_dim), padding=(2, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.dropout3 = nn.Dropout(0.25) \n",
    "        \n",
    "        # Post PCAT Conv 1\n",
    "        self.final_conv = nn.Conv2d(3 * 256, 1024, (3, 3), padding=(1, 1))\n",
    "        \n",
    "        self.bn_final = nn.BatchNorm2d(1024)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 2\n",
    "        self.conv_pcat2 = nn.Conv2d(1024, 2048, (3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat2 = nn.BatchNorm2d(2048)\n",
    "        self.pool_pcat2 = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 3\n",
    "        self.conv_pcat3 = nn.Conv2d(2048, 4096, (3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat3 = nn.BatchNorm2d(4096)\n",
    "        self.pool_pcat3 = nn.MaxPool2d((4, 1))\n",
    "        self.dropout_pcat3 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan_pre = KAN([32768, 256], num_grids=4)\n",
    "        self.kan = KAN([256, 64, 8, 2], num_grids=8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 100, seq_len-0, 1)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = x1.squeeze(3)  # shape: (batch_size, 100, seq_len-0)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 100, (seq_len-0)//2)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 100, seq_len-2, 1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = x2.squeeze(3)  # shape: (batch_size, 100, seq_len-2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 100, (seq_len-2)//2)\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 100, seq_len-4, 1)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = x3.squeeze(3)  # shape: (batch_size, 100, seq_len-4)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 100, (seq_len-4)//2)\n",
    "        x3 = self.dropout3(x3) \n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 300, ...)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 300, ..., 1)\n",
    "        \n",
    "        # PCAT1 Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 100, ..., 1)\n",
    "        x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "\n",
    "        x_out = self.dropout_pcat1(x_out)\n",
    "\n",
    "        # PCAT2 Conv2D layer\n",
    "        x_out = self.conv_pcat2(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        x_out = self.bn_pcat2(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat2(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "\n",
    "        # PCAT3 Conv2D layer\n",
    "        x_out = self.conv_pcat3(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        x_out = self.bn_pcat3(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat3(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 100)\n",
    "        x_out = self.dropout_pcat3(x_out)\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan_pre(x_out)\n",
    "        x_out = self.kan(x_out)# shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 32  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchinfo import summary\n",
    "#summary(model, input_size=(batch_size, max_l), dtypes=([torch.int64]), col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct += calculate_accuracy(outputs, labels)\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.RAdam(model.parameters(), lr=4e-3, weight_decay=1e-4, decoupled_weight_decay=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=8e-4, weight_decay=8e-5)\n",
    "#optimizer = optim.SparseAdam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Learning rate decay\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Accuracy calculation\n",
    "def calculate_accuracy(preds, labels):\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40\n",
    "model_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        for texts, labels in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct_predictions += calculate_accuracy(outputs, labels)\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            \n",
    "            pbar.set_postfix(loss=total_loss/total_predictions, accuracy=accuracy)\n",
    "    scheduler.step()\n",
    "\n",
    "    model_history.append(evaluate(model, test_loader))\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/(len(train_loader)*batch_size)}, Accuracy: {accuracy}')\n",
    "    print(f'Val_Loss: {model_history[epoch][1]}, Val_Accuracy: {model_history[epoch][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot val acc and loss\n",
    "accuracy = [entry[0] for entry in model_history]\n",
    "loss = [entry[1] for entry in model_history]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot accuracy on primary y\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy', color='tab:blue')\n",
    "ax1.plot(accuracy, label='Accuracy', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Plot the loss on secondary y\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Loss', color='tab:red')\n",
    "ax2.plot(loss, label='Loss', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "plt.title('Model Accuracy and Loss')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model_saves/model_complete.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc         # garbage collect library\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    " \n",
    "cuda.select_device(0) # choosing second GPU \n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "cuda.close()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
