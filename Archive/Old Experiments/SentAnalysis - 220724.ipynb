{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "1. ### Lemmatize input\n",
    "2. ### Replace CNNs with KAN-CNNs\n",
    "3. ### Prune and test\n",
    "4. ### B-spline Finetuning\n",
    "5. ### CNN operates on whole embedded lvl - test on various depths (3, 9, 27, 81)\n",
    "6. ### Deeper CNN - 1 + 3 + 5 gram feed into Deep CNN -> KAN\n",
    "7. ### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkan import FastKAN as KAN\n",
    "from kan_convs import KANConv2DLayer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from fastkan import FastKAN as KAN\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./imdb.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 1684567.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label\n",
       "0  One of the other reviewers has mentioned that ...  positive      1\n",
       "1  A wonderful little production. <br /><br />The...  positive      1\n",
       "2  I thought this was a wonderful way to spend ti...  positive      1\n",
       "3  Basically there's a family where a little boy ...  negative      0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to binary\n",
    "def transform_label(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "\n",
    "data['label'] = data['sentiment'].progress_apply(transform_label)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for dataset imbalance\n",
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 99280.89it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive      1   \n",
       "1  A wonderful little production. <br /><br />The...  positive      1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive      1   \n",
       "3  Basically there's a family where a little boy ...  negative      0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1   \n",
       "\n",
       "   token_length  \n",
       "0           307  \n",
       "1           162  \n",
       "2           166  \n",
       "3           138  \n",
       "4           230  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review token length\n",
    "data['token_length'] = data.review.progress_apply(lambda x: len(x.split()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review length\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    25000.000000\n",
       "mean       232.849320\n",
       "std        177.497046\n",
       "min         10.000000\n",
       "25%        125.000000\n",
       "50%        172.000000\n",
       "75%        284.000000\n",
       "max       2470.000000\n",
       "Name: token_length, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos = data[data['label'] == 1]\n",
    "print(\"Positive review length\")\n",
    "data_pos['token_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative review length\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    25000.000000\n",
       "mean       229.464560\n",
       "std        164.947795\n",
       "min          4.000000\n",
       "25%        128.000000\n",
       "50%        174.000000\n",
       "75%        278.000000\n",
       "max       1522.000000\n",
       "Name: token_length, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_neg = data[data['label'] == 0]\n",
    "print(\"Negative review length\")\n",
    "data_neg['token_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "    # \n",
    "    words = word_tokenize(text)\n",
    "    sent = [word for word in words if word not in stop_words]\n",
    "    sent = ' '.join(sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:23<00:00, 2085.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>token_length</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>basically family little boy jake thinks zombie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive      1   \n",
       "1  A wonderful little production. <br /><br />The...  positive      1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive      1   \n",
       "3  Basically there's a family where a little boy ...  negative      0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1   \n",
       "\n",
       "   token_length                                              clean  \n",
       "0           307  one reviewers mentioned watching 1 oz episode ...  \n",
       "1           162  wonderful little production filming technique ...  \n",
       "2           166  thought wonderful way spend time hot summer we...  \n",
       "3           138  basically family little boy jake thinks zombie...  \n",
       "4           230  petter mattei love time money visually stunnin...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "data['clean'] = data.review.progress_apply(preprocess_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text if necessary\n",
    "    words = text.split()\n",
    "    # Lemmatize each word in the text\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the lemmatized words back into a string\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:15<00:00, 3331.91it/s]\n"
     ]
    }
   ],
   "source": [
    "data['lemma'] = data['clean'].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one reviewers mentioned watching 1 oz episode hooked right exactly happened first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home many aryans muslims gangstas latinos christians italians irish scuffles death stares dodgy dealings shady agreements never far away would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards sold nickel inmates kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side\n",
      "one reviewer mentioned watching 1 oz episode hooked right exactly happened first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda em city home many aryan muslim gangsta latino christian italian irish scuffle death stare dodgy dealing shady agreement never far away would say main appeal show due fact go show dare forget pretty picture painted mainstream audience forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high level graphic violence violence injustice crooked guard sold nickel inmate kill order get away well mannered middle class inmate turned prison bitch due lack street skill prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side\n",
      "wonderful little production filming technique unassuming old time bbc fashion gives comforting sometimes discomforting sense realism entire piece actors extremely well chosen michael sheen got polari voices pat truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great master comedy life realism really comes home little things fantasy guard rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwell murals decorating every surface terribly well done\n",
      "wonderful little production filming technique unassuming old time bbc fashion give comforting sometimes discomforting sense realism entire piece actor extremely well chosen michael sheen got polari voice pat truly see seamless editing guided reference williams diary entry well worth watching terrificly written performed piece masterful production one great master comedy life realism really come home little thing fantasy guard rather use traditional dream technique remains solid disappears play knowledge sens particularly scene concerning orton halliwell set particularly flat halliwell mural decorating every surface terribly well done\n",
      "thought wonderful way spend time hot summer weekend sitting air conditioned theater watching light hearted comedy plot simplistic dialogue witty characters likable even well bread suspected serial killer may disappointed realize match point 2 risk addiction thought proof woody allen still fully control style many us grown love laughed one woody comedies years dare say decade never impressed scarlet johanson managed tone sexy image jumped right average spirited young woman may crown jewel career wittier devil wears prada interesting superman great comedy go see friends\n",
      "thought wonderful way spend time hot summer weekend sitting air conditioned theater watching light hearted comedy plot simplistic dialogue witty character likable even well bread suspected serial killer may disappointed realize match point 2 risk addiction thought proof woody allen still fully control style many u grown love laughed one woody comedy year dare say decade never impressed scarlet johanson managed tone sexy image jumped right average spirited young woman may crown jewel career wittier devil wear prada interesting superman great comedy go see friend\n",
      "basically family little boy jake thinks zombie closet parents fighting time movie slower soap opera suddenly jake decides become rambo kill zombie ok first going make film must decide thriller drama drama movie watchable parents divorcing arguing like real life jake closet totally ruins film expected see boogeyman similar movie instead watched drama meaningless thriller spots 3 10 well playing parents descent dialogs shots jake ignore\n",
      "basically family little boy jake think zombie closet parent fighting time movie slower soap opera suddenly jake decides become rambo kill zombie ok first going make film must decide thriller drama drama movie watchable parent divorcing arguing like real life jake closet totally ruin film expected see boogeyman similar movie instead watched drama meaningless thriller spot 3 10 well playing parent descent dialog shot jake ignore\n",
      "petter mattei love time money visually stunning film watch mr mattei offers us vivid portrait human relations movie seems telling us money power success people different situations encounter variation arthur schnitzler play theme director transfers action present time new york different characters meet connect one connected one way another next person one seems know previous point contact stylishly film sophisticated luxurious look taken see people live world live habitat thing one gets souls picture different stages loneliness one inhabits big city exactly best place human relations find sincere fulfillment one discerns case people encounter acting good mr mattei direction steve buscemi rosario dawson carol kane michael imperioli adrian grenier rest talented cast make characters come alive wish mr mattei good luck await anxiously next work\n",
      "petter mattei love time money visually stunning film watch mr mattei offer u vivid portrait human relation movie seems telling u money power success people different situation encounter variation arthur schnitzler play theme director transfer action present time new york different character meet connect one connected one way another next person one seems know previous point contact stylishly film sophisticated luxurious look taken see people live world live habitat thing one get soul picture different stage loneliness one inhabits big city exactly best place human relation find sincere fulfillment one discerns case people encounter acting good mr mattei direction steve buscemi rosario dawson carol kane michael imperioli adrian grenier rest talented cast make character come alive wish mr mattei good luck await anxiously next work\n",
      "probably time favorite movie story selflessness sacrifice dedication noble cause preachy boring never gets old despite seen 15 times last 25 years paul lukas performance brings tears eyes bette davis one truly sympathetic roles delight kids grandma says like dressed midgets children makes fun watch mother slow awakening happening world roof believable startling dozen thumbs movie\n",
      "probably time favorite movie story selflessness sacrifice dedication noble cause preachy boring never get old despite seen 15 time last 25 year paul lukas performance brings tear eye bette davis one truly sympathetic role delight kid grandma say like dressed midget child make fun watch mother slow awakening happening world roof believable startling dozen thumb movie\n",
      "sure would like see resurrection dated seahunt series tech today would bring back kid excitement grew black white tv seahunt gunsmoke hero every week vote comeback new sea hunt need change pace tv would work world water adventure oh way thank outlet like view many viewpoints tv many movies ole way believe got wan na say would nice read plus points sea hunt rhymes would 10 lines would let submit leave doubt quit must go lets\n",
      "sure would like see resurrection dated seahunt series tech today would bring back kid excitement grew black white tv seahunt gunsmoke hero every week vote comeback new sea hunt need change pace tv would work world water adventure oh way thank outlet like view many viewpoint tv many movie ole way believe got wan na say would nice read plus point sea hunt rhyme would 10 line would let submit leave doubt quit must go let\n",
      "show amazing fresh innovative idea 70 first aired first 7 8 years brilliant things dropped 1990 show really funny anymore continued decline complete waste time today truly disgraceful far show fallen writing painfully bad performances almost bad mildly entertaining respite guest hosts show probably still air find hard believe creator hand selected original cast also chose band hacks followed one recognize brilliance see fit replace mediocrity felt must give 2 stars respect original cast made show huge success show awful believe still air\n",
      "show amazing fresh innovative idea 70 first aired first 7 8 year brilliant thing dropped 1990 show really funny anymore continued decline complete waste time today truly disgraceful far show fallen writing painfully bad performance almost bad mildly entertaining respite guest host show probably still air find hard believe creator hand selected original cast also chose band hack followed one recognize brilliance see fit replace mediocrity felt must give 2 star respect original cast made show huge success show awful believe still air\n",
      "encouraged positive comments film looking forward watching film bad mistake seen 950 films truly one worst awful almost every way editing pacing storyline acting soundtrack film song lame country tune played less four times film looks cheap nasty boring extreme rarely happy see end credits film thing prevents giving 1 score harvey keitel far best performance least seems making bit effort one keitel obsessives\n",
      "encouraged positive comment film looking forward watching film bad mistake seen 950 film truly one worst awful almost every way editing pacing storyline acting soundtrack film song lame country tune played le four time film look cheap nasty boring extreme rarely happy see end credit film thing prevents giving 1 score harvey keitel far best performance least seems making bit effort one keitel obsessive\n",
      "like original gut wrenching laughter like movie young old love movie hell even mom liked great camp\n",
      "like original gut wrenching laughter like movie young old love movie hell even mom liked great camp\n",
      "phil alien one quirky films humour based around oddness everything rather actual punchlines first odd pretty funny movie progressed find jokes oddness funny anymore low budget film thats never problem pretty interesting characters eventually lost interest imagine film would appeal stoner currently partaking something similar better try brother another planet\n",
      "phil alien one quirky film humour based around oddness everything rather actual punchlines first odd pretty funny movie progressed find joke oddness funny anymore low budget film thats never problem pretty interesting character eventually lost interest imagine film would appeal stoner currently partaking something similar better try brother another planet\n",
      "saw movie 12 came recall scariest scene big bird eating men dangling helplessly parachutes right air horror horror young kid going cheesy b films saturday afternoons still tired formula monster type movies usually included hero beautiful woman might daughter professor happy resolution monster died end care much romantic angle 12 year old predictable plots love unintentional humor year later saw psycho came loved star janet leigh bumped early film sat took notice point since screenwriters making story make scary possible well worn formula rules\n",
      "saw movie 12 came recall scariest scene big bird eating men dangling helplessly parachute right air horror horror young kid going cheesy b film saturday afternoon still tired formula monster type movie usually included hero beautiful woman might daughter professor happy resolution monster died end care much romantic angle 12 year old predictable plot love unintentional humor year later saw psycho came loved star janet leigh bumped early film sat took notice point since screenwriter making story make scary possible well worn formula rule\n",
      "im big fan boll work many enjoyed movie postal maybe im one boll apparently bought rights use far cry long ago even game even finsished people enjoyed killing mercs infiltrating secret research labs located tropical island warned far cry something mr boll schemed together along legion schmucks feeling loneley set mr boll invites three countrymen play players go names til schweiger udo kier ralf moeller three names actually made selfs pretty big movie biz tale goes like jack carver played til schweiger yes carver german hail bratwurst eating dudes however find tils acting movie pretty badass people complained really staying true whole carver agenda saw carver first person perspective really know looked like kicking however storyline film beyond demented see evil mad scientist dr krieger played udo kier making genetically mutated soldiers gms called performing top secret research island reminds spoiler vancouver reason thats right palm trees instead got nice rich lumberjack woods even gone far started cry mehehe go wan na stay true bolls shenanigans go see movie disappointed delivers true boll experience meaning suck things worth mentioning would imply boll good work areas film nice boat fighting scenes whole cromed albino gms squad enters scene everything makes laugh movie far cry reeks scheisse poop simpletons fa r wan na take wiff go ahead btw carver gets annoying sidekick makes wan na shoot first three minutes screen\n",
      "im big fan boll work many enjoyed movie postal maybe im one boll apparently bought right use far cry long ago even game even finsished people enjoyed killing mercs infiltrating secret research lab located tropical island warned far cry something mr boll schemed together along legion schmuck feeling loneley set mr boll invite three countryman play player go name til schweiger udo kier ralf moeller three name actually made self pretty big movie biz tale go like jack carver played til schweiger yes carver german hail bratwurst eating dude however find tils acting movie pretty badass people complained really staying true whole carver agenda saw carver first person perspective really know looked like kicking however storyline film beyond demented see evil mad scientist dr krieger played udo kier making genetically mutated soldier gm called performing top secret research island reminds spoiler vancouver reason thats right palm tree instead got nice rich lumberjack wood even gone far started cry mehehe go wan na stay true boll shenanigan go see movie disappointed delivers true boll experience meaning suck thing worth mentioning would imply boll good work area film nice boat fighting scene whole cromed albino gm squad enters scene everything make laugh movie far cry reek scheisse poop simpleton fa r wan na take wiff go ahead btw carver get annoying sidekick make wan na shoot first three minute screen\n",
      "cast played shakespeare shakespeare lost appreciate trying bring shakespeare masses ruin something good scottish play favorite shakespeare know know certain rev bowdler hence bowdlerization tried something similar victorian era words improve perfection write write least ten lines text english composition never forte keep going say movie saying goes cut\n",
      "cast played shakespeare shakespeare lost appreciate trying bring shakespeare mass ruin something good scottish play favorite shakespeare know know certain rev bowdler hence bowdlerization tried something similar victorian era word improve perfection write write least ten line text english composition never forte keep going say movie saying go cut\n",
      "fantastic movie three prisoners become famous one actors george clooney fan roll bad another good thing movie soundtrack man constant sorrow recommand movie everybody greetings bart\n",
      "fantastic movie three prisoner become famous one actor george clooney fan roll bad another good thing movie soundtrack man constant sorrow recommand movie everybody greeting bart\n",
      "kind drawn erotic scenes realize one amateurish unbelievable bits film ever seen sort like high school film project rosanna arquette thinking stock characters bizarre supposed midwest town pretty hard get involved one lessons learned brilliant insights stilted quite ridiculous lots skin intrigues videotaped nonsense bisexual relationship nowhere heterosexual encounters absurd dance everybody playing stereotyped roles give one pass like million miles bad wasted film money could spent starving children aids africa\n",
      "kind drawn erotic scene realize one amateurish unbelievable bit film ever seen sort like high school film project rosanna arquette thinking stock character bizarre supposed midwest town pretty hard get involved one lesson learned brilliant insight stilted quite ridiculous lot skin intrigue videotaped nonsense bisexual relationship nowhere heterosexual encounter absurd dance everybody playing stereotyped role give one pas like million mile bad wasted film money could spent starving child aid africa\n",
      "films simply remade one bad film fails capture flavor terror 1963 film title liam neeson excellent always cast holds exception owen wilson bring right feel character luke major fault version strayed far shirley jackson story attempts grandiose lost thrill earlier film trade snazzier special effects say bad film enjoy friction terror older version much\n",
      "film simply remade one bad film fails capture flavor terror 1963 film title liam neeson excellent always cast hold exception owen wilson bring right feel character luke major fault version strayed far shirley jackson story attempt grandiose lost thrill earlier film trade snazzier special effect say bad film enjoy friction terror older version much\n",
      "movie made one top 10 awful movies horrible continuous minute fight one monster another chance character development busy running one sword fight another emotional attachment except big bad machine wanted destroy scenes blatantly stolen movies lotr star wars matrix examples ghost scene end stolen final scene old star wars yoda obee one vader spider machine beginning exactly like frodo attacked spider return kings elijah wood victim films wait hypnotizes stings victim wraps uh hello whole machine vs humans theme matrix terminator examples waste time someone tell nazi nazi juvenile story line rushed juvenile conclusion movie could decide children movie adult movie much either awful real disappointment say least save money\n",
      "movie made one top 10 awful movie horrible continuous minute fight one monster another chance character development busy running one sword fight another emotional attachment except big bad machine wanted destroy scene blatantly stolen movie lotr star war matrix example ghost scene end stolen final scene old star war yoda obee one vader spider machine beginning exactly like frodo attacked spider return king elijah wood victim film wait hypnotizes sting victim wrap uh hello whole machine v human theme matrix terminator example waste time someone tell nazi nazi juvenile story line rushed juvenile conclusion movie could decide child movie adult movie much either awful real disappointment say least save money\n",
      "remember film first film watched cinema picture dark places nervous back 74 75 dad took brother sister newbury cinema newbury berkshire england recall tigers lots snow film also appearance grizzly adams actor dan haggery think one tigers gets shot dies anyone knows find dvd etc please let know cinema turned fitness club big shame nearest cinema 20 miles away would love hear others seen film like\n",
      "remember film first film watched cinema picture dark place nervous back 74 75 dad took brother sister newbury cinema newbury berkshire england recall tiger lot snow film also appearance grizzly adam actor dan haggery think one tiger get shot dy anyone know find dvd etc please let know cinema turned fitness club big shame nearest cinema 20 mile away would love hear others seen film like\n",
      "awful film must real stinkers nominated golden globe taken story first famous female renaissance painter mangled beyond recognition complaint taken liberties facts story good would perfectly fine simply bizarre accounts true story artist would made far better film come dishwater dull script suppose enough naked people factual version hurriedly capped end summary artist life could saved couple hours favored rest film brevity\n",
      "awful film must real stinker nominated golden globe taken story first famous female renaissance painter mangled beyond recognition complaint taken liberty fact story good would perfectly fine simply bizarre account true story artist would made far better film come dishwater dull script suppose enough naked people factual version hurriedly capped end summary artist life could saved couple hour favored rest film brevity\n"
     ]
    }
   ],
   "source": [
    "for n in range(20):\n",
    "    print(data['clean'][n])\n",
    "    print(data['lemma'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:30<00:00, 1660.06it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_l = 256\n",
    "data['tokenized'] = data.lemma.progress_apply(lambda x: tokenizer.encode(text=x, add_special_tokens=False, truncation=True, add_prefix_space=True, padding='max_length', max_length=max_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 1117378.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    25000.0\n",
       "mean       256.0\n",
       "std          0.0\n",
       "min        256.0\n",
       "25%        256.0\n",
       "50%        256.0\n",
       "75%        256.0\n",
       "max        256.0\n",
       "Name: ct_length, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check actual tokenized text len\n",
    "data['ct_length'] = data.tokenized.progress_apply(lambda x: len(x))\n",
    "data_pos = data[data['label'] == 1]\n",
    "data_pos['ct_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X = data.tokenized\n",
    "y = data.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.tolist()).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.tolist()).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.tolist()).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.tolist()).to(device)\n",
    "\n",
    "# Create Datasets - tensors\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the data\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (embedding): Embedding(50257, 64)\n",
       "  (conv1): KANConv2DLayer(\n",
       "    (base_activation): GELU(approximate='none')\n",
       "    (base_conv): ModuleList(\n",
       "      (0): Conv2d(1, 32, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (spline_conv): ModuleList(\n",
       "      (0): Conv2d(8, 32, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (layer_norm): ModuleList(\n",
       "      (0): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (prelus): ModuleList(\n",
       "      (0): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (conv2): KANConv2DLayer(\n",
       "    (base_activation): GELU(approximate='none')\n",
       "    (base_conv): ModuleList(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 64), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "    )\n",
       "    (spline_conv): ModuleList(\n",
       "      (0): Conv2d(8, 32, kernel_size=(3, 64), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "    )\n",
       "    (layer_norm): ModuleList(\n",
       "      (0): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (prelus): ModuleList(\n",
       "      (0): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (conv3): KANConv2DLayer(\n",
       "    (base_activation): GELU(approximate='none')\n",
       "    (base_conv): ModuleList(\n",
       "      (0): Conv2d(1, 32, kernel_size=(5, 64), stride=(1, 1), padding=(2, 0), bias=False)\n",
       "    )\n",
       "    (spline_conv): ModuleList(\n",
       "      (0): Conv2d(8, 32, kernel_size=(5, 64), stride=(1, 1), padding=(2, 0), bias=False)\n",
       "    )\n",
       "    (layer_norm): ModuleList(\n",
       "      (0): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (prelus): ModuleList(\n",
       "      (0): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  (final_conv): KANConv2DLayer(\n",
       "    (base_activation): GELU(approximate='none')\n",
       "    (base_conv): ModuleList(\n",
       "      (0): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (spline_conv): ModuleList(\n",
       "      (0): Conv2d(768, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (layer_norm): ModuleList(\n",
       "      (0): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (prelus): ModuleList(\n",
       "      (0): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (bn_final): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool_final): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout_pcat1): Dropout(p=0.25, inplace=False)\n",
       "  (conv_pcat2): KANConv2DLayer(\n",
       "    (base_activation): GELU(approximate='none')\n",
       "    (base_conv): ModuleList(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (spline_conv): ModuleList(\n",
       "      (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (layer_norm): ModuleList(\n",
       "      (0): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (prelus): ModuleList(\n",
       "      (0): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (bn_pcat2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool_pcat2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout_pcat2): Dropout(p=0.25, inplace=False)\n",
       "  (conv_pcat3): KANConv2DLayer(\n",
       "    (base_activation): GELU(approximate='none')\n",
       "    (base_conv): ModuleList(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (spline_conv): ModuleList(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (layer_norm): ModuleList(\n",
       "      (0): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "    (prelus): ModuleList(\n",
       "      (0): PReLU(num_parameters=1)\n",
       "    )\n",
       "  )\n",
       "  (bn_pcat3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool_pcat3): MaxPool2d(kernel_size=(4, 1), stride=(4, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout_pcat3): Dropout(p=0.25, inplace=False)\n",
       "  (kan): FastKAN(\n",
       "    (layers): ModuleList(\n",
       "      (0): FastKANLayer(\n",
       "        (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (rbf): RadialBasisFunction()\n",
       "        (spline_linear): SplineLinear(in_features=3072, out_features=64, bias=False)\n",
       "        (base_linear): Linear(in_features=768, out_features=64, bias=True)\n",
       "      )\n",
       "      (1): FastKANLayer(\n",
       "        (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (rbf): RadialBasisFunction()\n",
       "        (spline_linear): SplineLinear(in_features=256, out_features=2, bias=False)\n",
       "        (base_linear): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# self.conv1 = KAN_Convolutional_Layer(n_convs = 5, kernel_size= (3,3), device = device)\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Conv layers with different kernel sizes\n",
    "        #self.conv1 = nn.Conv2d(1, 256, (1, embed_dim), padding=(0, 0))\n",
    "        #KANConv2DLayer(input_channels, layer_sizes[0], spline_order, kernel_size=3, groups=1, padding=0, stride=1, dilation=1),\n",
    "        self.conv1 = KANConv2DLayer(1, 32, spline_order=3, kernel_size=(1, embed_dim), padding=(0, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.1) \n",
    "        \n",
    "        #self.conv2 = nn.Conv2d(1, 256, (3, embed_dim), padding=(1, 0))\n",
    "        self.conv2 = KANConv2DLayer(1, 32, spline_order=3, kernel_size=(3, embed_dim), padding=(1, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.1) \n",
    "        \n",
    "        #self.conv3 = nn.Conv2d(1, 256, (5, embed_dim), padding=(2, 0))\n",
    "        self.conv3 = KANConv2DLayer(1, 32, spline_order=3, kernel_size=(5, embed_dim), padding=(2, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.dropout3 = nn.Dropout(0.1) \n",
    "        \n",
    "        # Post PCAT Conv 1\n",
    "        #self.final_conv = nn.Conv2d(3 * 64, 256, (3, 3), padding=(1, 1))\n",
    "        self.final_conv = KANConv2DLayer(3 * 32, 12, spline_order=3, kernel_size=(3, 3), padding=(1, 1))\n",
    "        \n",
    "        self.bn_final = nn.BatchNorm2d(128)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 2\n",
    "        #self.conv_pcat2 = nn.Conv2d(256, 512, (3, 3), padding=(1, 1))\n",
    "        self.conv_pcat2 = KANConv2DLayer(64, 128, spline_order=3, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat2 = nn.BatchNorm2d(128)\n",
    "        self.pool_pcat2 = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 3\n",
    "        #self.conv_pcat3 = nn.Conv2d(512, 2048, (3, 3), padding=(1, 1))\n",
    "        self.conv_pcat3 = KANConv2DLayer(128, 256, spline_order=3, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat3 = nn.BatchNorm2d(256)\n",
    "        self.pool_pcat3 = nn.MaxPool2d((4, 1))\n",
    "        self.dropout_pcat3 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([768, 64, 2], num_grids=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 100, seq_len-0, 1)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = x1.squeeze(3)  # shape: (batch_size, 100, seq_len-0)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 100, (seq_len-0)//2)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 100, seq_len-2, 1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = x2.squeeze(3)  # shape: (batch_size, 100, seq_len-2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 100, (seq_len-2)//2)\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 100, seq_len-4, 1)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = x3.squeeze(3)  # shape: (batch_size, 100, seq_len-4)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 100, (seq_len-4)//2)\n",
    "        x3 = self.dropout3(x3) \n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 300, ...)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 300, ..., 1)\n",
    "        \n",
    "        # PCAT1 Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 100, ..., 1)\n",
    "        #x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "\n",
    "        #x_out = self.dropout_pcat1(x_out)\n",
    "\n",
    "        # # PCAT2 Conv2D layer\n",
    "        # x_out = self.conv_pcat2(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        # x_out = self.bn_pcat2(x_out)\n",
    "        # x_out = F.relu(x_out)\n",
    "        # x_out = self.pool_pcat2(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "\n",
    "        # # PCAT3 Conv2D layer\n",
    "        # x_out = self.conv_pcat3(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        # x_out = self.bn_pcat3(x_out)\n",
    "        # x_out = F.relu(x_out)\n",
    "        # x_out = self.pool_pcat3(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 100)\n",
    "        x_out = self.dropout_pcat3(x_out)\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan(x_out)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct += calculate_accuracy(outputs, labels)\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1250/1250 [02:25<00:00,  8.61it/s, accuracy=0.508, loss=0.0224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7155479764938355, Accuracy: 0.507725\n",
      "Val_Loss: 0.021808627510070802, Val_Accuracy: 0.4976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1250/1250 [02:24<00:00,  8.63it/s, accuracy=0.538, loss=0.0216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6897578208446503, Accuracy: 0.538225\n",
      "Val_Loss: 0.022967179489135743, Val_Accuracy: 0.5105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1250/1250 [02:25<00:00,  8.61it/s, accuracy=0.646, loss=0.0195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6226537287473679, Accuracy: 0.64635\n",
      "Val_Loss: 0.016190487721562386, Val_Accuracy: 0.7459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1250/1250 [02:24<00:00,  8.63it/s, accuracy=0.784, loss=0.0145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.46325190124511717, Accuracy: 0.783925\n",
      "Val_Loss: 0.014209339386224746, Val_Accuracy: 0.7919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 1250/1250 [02:25<00:00,  8.61it/s, accuracy=0.847, loss=0.011] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.3531745821714401, Accuracy: 0.847425\n",
      "Val_Loss: 0.013525442057847977, Val_Accuracy: 0.8164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 1250/1250 [02:25<00:00,  8.61it/s, accuracy=0.884, loss=0.00887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.28380159620940687, Accuracy: 0.8843\n",
      "Val_Loss: 0.010609718772768974, Val_Accuracy: 0.8552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 1250/1250 [02:24<00:00,  8.63it/s, accuracy=0.907, loss=0.00735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.2352292162179947, Accuracy: 0.906975\n",
      "Val_Loss: 0.01314443046450615, Val_Accuracy: 0.8296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 1250/1250 [02:24<00:00,  8.64it/s, accuracy=0.922, loss=0.00629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.20130193938910962, Accuracy: 0.92205\n",
      "Val_Loss: 0.012296551925688981, Val_Accuracy: 0.8492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 1250/1250 [02:25<00:00,  8.61it/s, accuracy=0.934, loss=0.00531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.16982176019400358, Accuracy: 0.9344\n",
      "Val_Loss: 0.012376628337055445, Val_Accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 1250/1250 [02:24<00:00,  8.64it/s, accuracy=0.942, loss=0.00481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.153988341332227, Accuracy: 0.942025\n",
      "Val_Loss: 0.012578929792344571, Val_Accuracy: 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 1250/1250 [02:24<00:00,  8.64it/s, accuracy=0.95, loss=0.00408] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.13061086401455105, Accuracy: 0.95045\n",
      "Val_Loss: 0.01365750485509634, Val_Accuracy: 0.8505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:  51%|█████     | 636/1250 [01:12<01:09,  8.82it/s, accuracy=0.967, loss=0.00292]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 34\u001B[0m\n\u001B[0;32m     31\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     32\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 34\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m correct_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m calculate_accuracy(outputs, labels)\n\u001B[0;32m     36\u001B[0m total_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate decay\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.975)\n",
    "\n",
    "# Accuracy calculation\n",
    "def calculate_accuracy(preds, labels):\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "model_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        for texts, labels in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct_predictions += calculate_accuracy(outputs, labels)\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            \n",
    "            pbar.set_postfix(loss=total_loss/total_predictions, accuracy=accuracy)\n",
    "    scheduler.step()\n",
    "\n",
    "    model_history.append(evaluate(model, test_loader))\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}, Accuracy: {accuracy}')\n",
    "    print(f'Val_Loss: {model_history[epoch][1]}, Val_Accuracy: {model_history[epoch][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot val acc and loss\n",
    "accuracy = [entry[0] for entry in model_history]\n",
    "loss = [entry[1] for entry in model_history]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot accuracy on primary y\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy', color='tab:blue')\n",
    "ax1.plot(accuracy, label='Accuracy', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Plot the loss on secondary y\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Loss', color='tab:red')\n",
    "ax2.plot(loss, label='Loss', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "plt.title('Model Accuracy and Loss')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "\n",
    "import gc         # garbage collect library\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "class SentimentCNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(SentimentCNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # convolutional layers with batch Normalization\n",
    "        self.conv1 = nn.Conv2d(1, 100, (1, embed_dim), padding=(0, 0))  # 1-gram\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1, 100, (3, embed_dim), padding=(1, 0))  # 3-gram\n",
    "        self.bn2 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(1, 100, (5, embed_dim), padding=(2, 0))  # 5-gram\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.conv_pcat1 = nn.Conv2d(1, 200, (3, 3), padding=(0,0))\n",
    "        self.bn_pcat1 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([37500, 512, 128, num_class], num_grids=4)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        conv1_out = F.relu(self.bn1(self.conv1(embedded))).squeeze(3)\n",
    "        conv2_out = F.relu(self.bn2(self.conv2(embedded))).squeeze(3)\n",
    "        conv3_out = F.relu(self.bn3(self.conv3(embedded))).squeeze(3)\n",
    "        \n",
    "        pooled1 = F.max_pool1d(conv1_out, 2).squeeze(2)\n",
    "        pooled2 = F.max_pool1d(conv2_out, 2).squeeze(2)\n",
    "        pooled3 = F.max_pool1d(conv3_out, 2).squeeze(2)\n",
    "\n",
    "        cat = torch.cat((pooled1, pooled2, pooled3), 0)\n",
    "\n",
    "        conv_pcat1_out = F.relu(self.bn_pcat1(self.conv_pcat1(cat))).squeeze(3)\n",
    "        pooled_pcat1 = F.max_pool2d(conv1_out, 2).squeeze(2)\n",
    "        \n",
    "        #cat = cat.view(cat.size(0), -1)\n",
    "        dropout = self.dropout(pooled_pcat1)\n",
    "        \n",
    "        # Use kan for final classification\n",
    "        out = self.kan(dropout)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 128  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = SentimentCNNModel(vocab_size, embed_dim, num_class)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89.0% Val\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Conv layers with different kernel sizes\n",
    "        self.conv1 = nn.Conv2d(1, 100, (1, embed_dim), padding=(0, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1, 100, (3, embed_dim), padding=(1, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(100)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(1, 100, (5, embed_dim), padding=(2, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Final Conv2D layer after concatenation\n",
    "        self.final_conv = nn.Conv2d(3 * 100, 500, (3, 3), padding=(1, 1))\n",
    "        \n",
    "        # Additional layers\n",
    "        self.bn_final = nn.BatchNorm2d(100)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout = nn.Dropout(0.65)\n",
    "        \n",
    "        # Dense layer\n",
    "        #self.fc = nn.Linear(100, num_classes)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([32000, 512, 128, 2], num_grids=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 100, seq_len-0, 1)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = x1.squeeze(3)  # shape: (batch_size, 100, seq_len-0)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 100, (seq_len-0)//2)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 100, seq_len-2, 1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = x2.squeeze(3)  # shape: (batch_size, 100, seq_len-2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 100, (seq_len-2)//2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 100, seq_len-4, 1)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = x3.squeeze(3)  # shape: (batch_size, 100, seq_len-4)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 100, (seq_len-4)//2)\n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 300, ...)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 300, ..., 1)\n",
    "        \n",
    "        # Final Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 100, ..., 1)\n",
    "        #x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 100)\n",
    "        x_out = self.dropout(x_out)\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan(x_out)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 128  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/1250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'closure'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 31\u001B[0m\n\u001B[0;32m     29\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     30\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 31\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     34\u001B[0m correct_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m calculate_accuracy(outputs, labels)\n",
      "File \u001B[1;32md:\\python22-23\\imgrecfastkan\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    386\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    388\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    389\u001B[0m             )\n\u001B[1;32m--> 391\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    394\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32md:\\python22-23\\imgrecfastkan\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mTypeError\u001B[0m: step() missing 1 required positional argument: 'closure'"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate decay\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Accuracy calculation\n",
    "def calculate_accuracy(preds, labels):\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "model_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        for texts, labels in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct_predictions += calculate_accuracy(outputs, labels)\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            \n",
    "            pbar.set_postfix(loss=total_loss/total_predictions, accuracy=accuracy)\n",
    "    scheduler.step()\n",
    "\n",
    "    model_history.append(evaluate(model, test_loader))\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}, Accuracy: {accuracy}')\n",
    "    print(f'Val_Loss: {model_history[epoch][1]}, Val_Accuracy: {model_history[epoch][0]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}